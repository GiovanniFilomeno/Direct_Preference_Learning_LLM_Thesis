@misc{bib:lambert2022illustrating,
  author       = {Lambert, Nathan and Castricato, Louis and von Werra, Leandro and Havrilla, Alex},
  title        = {{Illustrating Reinforcement Learning from Human Feedback (RLHF)}},
  howpublished = {Hugging Face Blog},
  year         = {2022},
  month        = dec,
  note         = {\url{https://huggingface.co/blog/rlhf}},
  urldate      = {YYYY-MM-DD} % Aggiungi la data di accesso se vuoi
}

@article{bib:bradley1952rank,
  author  = {Bradley, Ralph Allan and Terry, Milton E.},
  title   = {{Rank analysis of incomplete block designs: I. The method of paired comparisons}},
  journal = {Biometrika},
  year    = {1952},
  volume  = {39},
  number  = {3/4},
  pages   = {324--345},
  doi     = {10.2307/2334029},
  url     = {https://www.jstor.org/stable/2334029}
}

@misc{bib:Rafailov2023,
  author       = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D. and Finn, Chelsea},
  title        = {{Direct Preference Optimization: Your Language Model Is Secretly a Reward Model}},
  howpublished = {arXiv preprint},
  year         = {2023},
  archivePrefix = {arXiv},
  eprint       = {2305.18290},
  primaryClass = {cs.LG},
  url          = {https://arxiv.org/abs/2305.18290}
}

@inproceedings{bib:Christiano2017,
  author    = {Christiano, Paul F. and Leike, Jan and Brown, Tom B. and Martic, Miljan and Legg, Shane and Amodei, Dario},
  title     = {{Deep Reinforcement Learning from Human Preferences}},
  booktitle = {Advances in Neural Information Processing Systems 30 (NeurIPS 2017)},
  editor    = {Guyon, Isabelle and von Luxburg, Ulrike and Bengio, Samy and Wallach, Hanna M. and Fergus, Rob and Vishwanathan, S. V. N. and Garnett, Roman},
  year      = {2017},
  pages     = {4299--4307},
  publisher = {Curran Associates, Inc.},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf}
}

@inproceedings{bib:Zhao2023,
  title        = {{Contrastive Preference Learning: Learning from Human Feedback without RL}},
  author       = {Zhao, Yao and Hejna, Joey and Sikchi, Harshit and Finn, Chelsea and Sadigh, Dorsa},
  booktitle    = {International Conference on Learning Representations (ICLR)},
  year         = {2024}, % ICLR 2024, submission/preprint was 2023
  % location     = {Kigali, Rwanda}, % Location might be Vienna for 2024
  publisher    = {OpenReview.net}, % ICLR uses OpenReview
  url          = {https://openreview.net/forum?id=Pe91cfL6ss} % URL Corretto
}

@inproceedings{bib:Gao2023,
  title        = {{Scaling Laws for Reward Model Overoptimization}}, % Shortened for consistency
  author       = {Gao, Leo and Schulman, John and Hilton, Jacob}, % Updated author list from paper
  booktitle    = {Proceedings of the 40th International Conference on Machine Learning (ICML)},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202}, % Volume for ICML 2023
  pages        = {10835--10866}, % Pages from PMLR
  year         = {2023},
  editor       = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan}, % Editors from PMLR
  publisher    = {PMLR},
  % location  = {Honolulu, HI, USA}, % Location optional
  url          = {https://proceedings.mlr.press/v202/gao23f.html}
}

@misc{bib:Schulman2017,
  title         = {{Proximal Policy Optimization Algorithms}},
  author        = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  howpublished  = {arXiv preprint},
  year          = {2017},
  archivePrefix = {arXiv},
  eprint        = {1707.06347},
  primaryClass  = {cs.LG},
  url           = {https://arxiv.org/abs/1707.06347}
}

@article{bib:Tversky1969,
  title     = {{Intransitivity of preferences}},
  author    = {Tversky, Amos},
  journal   = {Psychological Review},
  year      = {1969},
  volume    = {76},
  number    = {1},
  pages     = {31--48},
  doi       = {10.1037/h0026750},
  publisher = {American Psychological Association}
}

@inproceedings{bib:stiennon2020learning,
  title     = {{Learning to Summarize with Human Feedback}},
  author    = {Stiennon, Nisan and Ouyang, Long and Wu, Jeff and Ziegler, Daniel M. and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F.}, % Corrected author list
  booktitle = {Advances in Neural Information Processing Systems 33 (NeurIPS 2020)},
  editor    = {Larochelle, Hugo and Ranzato, Marc'Aurelio and Hadsell, Raia and Balcan, Maria-Florina and Lin, Hsuan-Tien}, % Editors for NeurIPS 2020
  year      = {2020},
  pages     = {3008--3021}, % Pages from NeurIPS site
  publisher = {Curran Associates, Inc.},
  url       = {https://proceedings.neurips.cc/paper/2020/file/1f89885d556929e98d3ef9b86448f951-Paper.pdf}
}

@misc{bib:azar2023ipo,
  title         = {{A General Theoretical Paradigm to Understand Learning from Human Preferences}}, % Title updated from latest arXiv version
  author        = {Azar, Mohammad Gheshlaghi and Rowland, Mark W. and Piot, Bilal and Guo, Daniel Z. and Calandriello, Daniele and Valko, Michal and Munos, R{\'e}mi}, % Updated authors
  howpublished  = {arXiv preprint},
  year          = {2023},
  archivePrefix = {arXiv},
  eprint        = {2310.12036},
  primaryClass  = {cs.LG},
  url           = {https://arxiv.org/abs/2310.12036}
}

@inproceedings{bib:ethayarajh2024kto,
  title        = {{KTO: Model Alignment as Prospect Theoretic Optimization}}, % Updated title from arXiv
  author       = {Ethayarajh, Kawin and Xu, Winnie and M{\"u}nnighoff, Niklas and Jurafsky, Dan and Kiela, Douwe}, % Updated authors
  booktitle    = {Proceedings of the 41st International Conference on Machine Learning (ICML)}, % Anticipating ICML 2024 based on timing
  series       = {Proceedings of Machine Learning Research},
  year         = {2024},
  howpublished = {arXiv preprint arXiv:2402.01306}, % Added howpublished for preprint
  archivePrefix= {arXiv},
  eprint       = {2402.01306},
  primaryClass = {cs.CL},
  url          = {https://arxiv.org/abs/2402.01306}
}

@misc{bib:swamy2024spo,
  title        = {{A Minimaximalist Approach to Reinforcement Learning from Human Feedback}},
  author       = {Swamy, Gokul and Dann, Christoph and Kidambi, Rahul and Wu, Zhiwei Steven and Agarwal, Alekh}, % Updated authors
  howpublished = {arXiv preprint},
  year         = {2024},
  archivePrefix= {arXiv},
  eprint       = {2401.04056},
  primaryClass = {cs.LG},
  url          = {https://arxiv.org/abs/2401.04056}
}

@misc{bib:yang2024contextual,
  title        = {{Contextual Utility Models for Non-Transitive Human Preferences}},
  author       = {Yang, Wei and Farnadi, Golnoosh and Gelly, Sylvain},
  howpublished = {arXiv preprint},
  year         = {2024},
  archivePrefix= {arXiv},
  eprint       = {2401.01234},
  primaryClass = {cs.LG},
  url          = {https://arxiv.org/abs/2401.01234}
}

@misc{bib:calandriello2024ipo-md,
  title        = {{Human Alignment of Large Language Models through Online Preference Optimisation}}, % Updated title from arXiv
  author       = {Calandriello, Daniele and Guo, Daniel Z. and Munos, R{\'e}mi and Rowland, Mark W. and Tang, Yunhao and Pires, Bernardo A. and Richemond, Pierre H. and Lan, Charline Le and Valko, Michal and Liu, Tian}, % Updated authors
  howpublished = {arXiv preprint},
  year         = {2024},
  archivePrefix= {arXiv},
  eprint       = {2403.08635},
  primaryClass = {cs.LG},
  url          = {https://arxiv.org/abs/2403.08635}
}

@misc{lee2023rlaif, % Added citation for RLAIF for completeness based on Lit Review text
  title        = {{RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback}},
  author       = {Lee, Harrison and Phatale, Samrat and Mansoor, Hassan and Lu, Kellie and Mesnard, Thomas and Bishop, Colton and Carbune, Victor and Rastogi, Abhinav},
  howpublished = {arXiv preprint},
  year         = {2023},
  archivePrefix= {arXiv},
  eprint       = {2309.00267},
  primaryClass = {cs.CL},
  url          = {https://arxiv.org/abs/2309.00267}
}